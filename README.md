
# ğŸ§  Evaluating the Comprehension of Technical Illustrations by Visual Language Models

This project introduces **TICQA** (Technical Illustration Comprehension Question Answering), a novel dataset designed to evaluate the ability of Vision-Language Models (VLMs) to understand **technical illustrations**. The study also benchmarks nine VLMs across three visual reasoning tasks and provides a detailed empirical analysis.

---

## ğŸ“Œ Motivation

Technical diagramsâ€”commonly found in textbooks, industrial manuals, and user guidesâ€”are essential forms of visual communication. Despite their importance, **no dedicated benchmark exists** to evaluate VLM performance on such illustrations. This work addresses that gap by:

- Creating a focused dataset on technical illustrations,
- Evaluating current VLM capabilities,
- Proposing a reproducible dataset generation methodology.

---

## ğŸ“¦ Contributions

1. **ğŸ—‚ TICQA Dataset**  
   A curated set of **250 samples**, divided into three task-specific subsets:
   - Visual Sequence Interpretation (assembly/disassembly)
   - Tools and Components Identification
   - Safety Warning Recognition

2. **ğŸ”„ Semi-Automatic Dataset Generation Pipeline**  
   The paper outlines a clear method for replicating and expanding dataset creation.

3. **ğŸ“Š Empirical Evaluation of Nine VLMs**  
   Comprehensive benchmarking of medium-sized open-source VLMs with task-specific accuracy metrics and insights.

---

## â“ Research Questions

1. How accurately can state-of-the-art VLMs infer **assembly/disassembly sequences** from diagrams when text labels are redacted?

2. Can VLMs correctly **identify tools/components** based solely on visual shape cues?

3. How effectively do VLMs **recognize and describe safety warnings** in open-ended formats?

---

## ğŸ§ª Results

- **Assembly MCQ Performance**: In many cases, VLMs matched or **surpassed human performance**.
- **Open-Ended Tasks**: VLMs struggled with **safety descriptions** and **component naming**.
- **Contextual Inputs**: On average, **+4.5% accuracy gain** with contextâ€”but task-dependent effects observed.

---

## ğŸ”® Future Work

- Expand the dataset **beyond 250 samples** to cover broader tasks and styles.
- Include **larger, state-of-the-art VLMs** for deeper comparative analysis.
- Conduct **in-depth behavior studies** to uncover training blind spots in current models.

---

## ğŸ§¬ Dataset Access

The TICQA dataset is publicly available on HuggingFace:  
ğŸ‘‰ [TICQA on Hugging Face](https://huggingface.co/datasets/SarangChouguley/TICQA)

---

## ğŸ“„ Project Structure

```
â”œâ”€â”€ scripts/ # Scripts to run the evaluation
â”œâ”€â”€ result/ # Evaluation results
â”œâ”€â”€ report/ # Final report and appendix
â””â”€â”€ README.md
```
---

## ğŸ“„ Citation

```
@misc{ticqa2025,
author = {Sarang Ravi Chouguley},
title = {Evaluating the Comprehension of Technical Illustrations by Visual Language Models},
year = {2025},
howpublished = {\url{https://github.com/SarangChougs/comprehension-of-technical-illustrations-by-VLMs}}
}
```

---

## ğŸ“¬ Contact

For questions, feedback, or collaboration:  
ğŸ“§ sarangchouguley284@gmail.com